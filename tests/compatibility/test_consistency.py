#!/usr/bin/env python3
"""
Clickzettaä¸€è‡´æ€§æµ‹è¯•

æµ‹è¯•clickzetta_difyé¡¹ç›®ä¸difyä¸»é¡¹ç›®çš„Clickzettaå®ç°æ˜¯å¦ä¸€è‡´
éªŒè¯è¡¨ç»“æ„ã€ç´¢å¼•åˆ›å»ºã€æ•°æ®æ“ä½œçš„å…¼å®¹æ€§
"""

import os
import sys
import json
import time
import uuid
from typing import List, Dict, Any
import numpy as np

# æ·»åŠ å·¥å…·è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'tools'))

try:
    from lakehouse_connection import LakehouseConnection
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥lakehouse_connection: {e}")
    sys.exit(1)

class ClickzettaConsistencyTest:
    """Clickzettaä¸€è‡´æ€§æµ‹è¯•ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.test_collection = f"test_consistency_{int(time.time())}"
        self.test_schema = "dify"
        self.test_dimension = 1536
        self.connection = None
        self.test_results = {}
        
        # æµ‹è¯•é…ç½®
        self.config = {
            "username": os.getenv("CLICKZETTA_USERNAME"),
            "password": os.getenv("CLICKZETTA_PASSWORD"),
            "instance": os.getenv("CLICKZETTA_INSTANCE"),
            "service": os.getenv("CLICKZETTA_SERVICE", "uat-api.clickzetta.com"),
            "workspace": os.getenv("CLICKZETTA_WORKSPACE", "quick_start"),
            "vcluster": os.getenv("CLICKZETTA_VCLUSTER", "default_ap"),
            "schema": self.test_schema
        }
        
        # éªŒè¯ç¯å¢ƒå˜é‡
        required_vars = ["username", "password", "instance"]
        missing_vars = [var for var in required_vars if not self.config.get(var)]
        if missing_vars:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {missing_vars}")
    
    def setup_connection(self) -> bool:
        """å»ºç«‹è¿æ¥"""
        try:
            conn_manager = LakehouseConnection()
            self.connection = conn_manager.get_connection(self.config)
            print("âœ… è¿æ¥å»ºç«‹æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            return False
    
    def test_table_structure_consistency(self) -> bool:
        """æµ‹è¯•è¡¨ç»“æ„ä¸€è‡´æ€§"""
        print("\nğŸ§ª æµ‹è¯•è¡¨ç»“æ„ä¸€è‡´æ€§...")
        
        try:
            with self.connection.cursor() as cursor:
                # åˆ›å»ºæµ‹è¯•è¡¨ (ä½¿ç”¨difyä¸»é¡¹ç›®çš„è¡¨ç»“æ„)
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS {self.test_schema}.{self.test_collection} (
                    id STRING NOT NULL,
                    page_content STRING NOT NULL,
                    metadata JSON,
                    vector VECTOR(FLOAT, {self.test_dimension}) NOT NULL,
                    PRIMARY KEY (id)
                )
                """
                cursor.execute(create_table_sql)
                print("âœ… è¡¨åˆ›å»ºæˆåŠŸï¼ˆä½¿ç”¨difyä¸»é¡¹ç›®ç»“æ„ï¼‰")
                
                # éªŒè¯è¡¨ç»“æ„
                cursor.execute(f"DESC {self.test_schema}.{self.test_collection}")
                columns = cursor.fetchall()
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ["id", "page_content", "metadata", "vector"]
                existing_fields = [col[0] for col in columns]
                
                missing_fields = [field for field in required_fields if field not in existing_fields]
                if missing_fields:
                    print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
                    return False
                
                print("âœ… è¡¨ç»“æ„éªŒè¯é€šè¿‡")
                print(f"   å­—æ®µåˆ—è¡¨: {', '.join(existing_fields)}")
                
                self.test_results['table_structure'] = True
                return True
                
        except Exception as e:
            print(f"âŒ è¡¨ç»“æ„æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['table_structure'] = False
            return False
    
    def test_index_creation_consistency(self) -> bool:
        """æµ‹è¯•ç´¢å¼•åˆ›å»ºä¸€è‡´æ€§"""
        print("\nğŸ§ª æµ‹è¯•ç´¢å¼•åˆ›å»ºä¸€è‡´æ€§...")
        
        try:
            with self.connection.cursor() as cursor:
                # åˆ›å»ºHNSWå‘é‡ç´¢å¼• (ä½¿ç”¨difyä¸»é¡¹ç›®çš„ç´¢å¼•å‚æ•°)
                vector_index_name = f"idx_{self.test_collection}_vector"
                
                vector_index_sql = f"""
                CREATE VECTOR INDEX IF NOT EXISTS {vector_index_name}
                ON TABLE {self.test_schema}.{self.test_collection}(vector)
                PROPERTIES (
                    "distance.function" = "cosine_distance",
                    "scalar.type" = "f32",
                    "m" = "16",
                    "ef.construction" = "128"
                )
                """
                cursor.execute(vector_index_sql)
                print("âœ… HNSWå‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸ")
                
                # åˆ›å»ºå€’æ’ç´¢å¼• (ç”¨äºå…¨æ–‡æœç´¢)
                text_index_name = f"idx_{self.test_collection}_text"
                
                text_index_sql = f"""
                CREATE INVERTED INDEX IF NOT EXISTS {text_index_name}
                ON TABLE {self.test_schema}.{self.test_collection} (page_content)
                PROPERTIES (
                    "analyzer" = "chinese",
                    "mode" = "smart"
                )
                """
                try:
                    cursor.execute(text_index_sql)
                    print("âœ… å€’æ’ç´¢å¼•åˆ›å»ºæˆåŠŸ")
                    inverted_index_created = True
                except Exception as e:
                    print(f"âš ï¸ å€’æ’ç´¢å¼•åˆ›å»ºå¤±è´¥ï¼ˆä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ï¼‰: {e}")
                    inverted_index_created = False
                
                # éªŒè¯ç´¢å¼•
                cursor.execute(f"SHOW INDEX FROM {self.test_schema}.{self.test_collection}")
                indexes = cursor.fetchall()
                
                index_names = [str(idx) for idx in indexes]
                vector_index_exists = any("vector" in idx.lower() for idx in index_names)
                
                if not vector_index_exists:
                    print("âŒ å‘é‡ç´¢å¼•æœªæ‰¾åˆ°")
                    return False
                
                print("âœ… ç´¢å¼•éªŒè¯é€šè¿‡")
                print(f"   ç´¢å¼•æ•°é‡: {len(indexes)}")
                
                self.test_results['index_creation'] = True
                return True
                
        except Exception as e:
            print(f"âŒ ç´¢å¼•åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
            self.test_results['index_creation'] = False
            return False
    
    def test_data_insertion_consistency(self) -> bool:
        """æµ‹è¯•æ•°æ®æ’å…¥ä¸€è‡´æ€§"""
        print("\nğŸ§ª æµ‹è¯•æ•°æ®æ’å…¥ä¸€è‡´æ€§...")
        
        try:
            with self.connection.cursor() as cursor:
                # å‡†å¤‡æµ‹è¯•æ•°æ®
                test_data = []
                for i in range(5):
                    doc_id = f"test_doc_{i+1}_{uuid.uuid4()}"
                    content = f"æµ‹è¯•æ–‡æ¡£ {i+1}: è¿™æ˜¯ä¸difyä¸»é¡¹ç›®ä¿æŒä¸€è‡´çš„æµ‹è¯•å†…å®¹ã€‚"
                    metadata = {
                        "doc_id": doc_id,
                        "document_id": f"doc_{i+1}",
                        "source": "consistency_test",
                        "index": i
                    }
                    vector = np.random.random(self.test_dimension).tolist()
                    test_data.append((doc_id, content, metadata, vector))
                
                # ä½¿ç”¨difyä¸»é¡¹ç›®çš„æ’å…¥æ ¼å¼
                values = []
                for doc_id, content, metadata, vector in test_data:
                    vector_str = f"VECTOR({','.join(map(str, vector))})"
                    metadata_str = json.dumps(metadata, ensure_ascii=False).replace("'", "''")
                    content_str = content.replace("'", "''")
                    
                    values.append(f"('{doc_id}', '{content_str}', JSON '{metadata_str}', {vector_str})")
                
                insert_sql = f"""
                INSERT INTO {self.test_schema}.{self.test_collection} (id, page_content, metadata, vector)
                VALUES {','.join(values)}
                """
                
                start_time = time.time()
                cursor.execute(insert_sql)
                insert_time = time.time() - start_time
                
                print(f"âœ… æ•°æ®æ’å…¥æˆåŠŸ: {len(test_data)} æ¡è®°å½•ï¼Œè€—æ—¶ {insert_time:.3f}s")
                
                # éªŒè¯æ’å…¥ç»“æœ
                cursor.execute(f"SELECT COUNT(*) FROM {self.test_schema}.{self.test_collection}")
                count = cursor.fetchone()[0]
                
                if count != len(test_data):
                    print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: æœŸæœ› {len(test_data)}ï¼Œå®é™… {count}")
                    return False
                
                print("âœ… æ•°æ®éªŒè¯é€šè¿‡")
                
                self.test_results['data_insertion'] = True
                return True
                
        except Exception as e:
            print(f"âŒ æ•°æ®æ’å…¥æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['data_insertion'] = False
            return False
    
    def test_vector_search_consistency(self) -> bool:
        """æµ‹è¯•å‘é‡æœç´¢ä¸€è‡´æ€§"""
        print("\nğŸ§ª æµ‹è¯•å‘é‡æœç´¢ä¸€è‡´æ€§...")
        
        try:
            with self.connection.cursor() as cursor:
                # ä½¿ç”¨difyä¸»é¡¹ç›®çš„æœç´¢æ ¼å¼
                query_vector = np.random.random(self.test_dimension).tolist()
                vector_str = f"VECTOR({','.join(map(str, query_vector))})"
                
                search_sql = f"""
                SELECT id, page_content, metadata,
                       COSINE_DISTANCE(vector, {vector_str}) AS distance
                FROM {self.test_schema}.{self.test_collection}
                ORDER BY distance
                LIMIT 3
                """
                
                start_time = time.time()
                cursor.execute(search_sql)
                results = cursor.fetchall()
                search_time = time.time() - start_time
                
                print(f"âœ… å‘é‡æœç´¢æˆåŠŸ: è¿”å› {len(results)} ä¸ªç»“æœï¼Œè€—æ—¶ {search_time*1000:.0f}ms")
                
                # éªŒè¯æœç´¢ç»“æœ
                for i, row in enumerate(results):
                    doc_id, content, metadata_json, distance = row
                    metadata = json.loads(metadata_json) if metadata_json else {}
                    print(f"   ç»“æœ {i+1}: è·ç¦»={distance:.4f}, æ–‡æ¡£ID={doc_id}")
                
                if len(results) == 0:
                    print("âŒ æœç´¢ç»“æœä¸ºç©º")
                    return False
                
                print("âœ… æœç´¢ç»“æœéªŒè¯é€šè¿‡")
                
                self.test_results['vector_search'] = True
                return True
                
        except Exception as e:
            print(f"âŒ å‘é‡æœç´¢æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['vector_search'] = False
            return False
    
    def test_full_text_search_consistency(self) -> bool:
        """æµ‹è¯•å…¨æ–‡æœç´¢ä¸€è‡´æ€§"""
        print("\nğŸ§ª æµ‹è¯•å…¨æ–‡æœç´¢ä¸€è‡´æ€§...")
        
        try:
            with self.connection.cursor() as cursor:
                # ä½¿ç”¨difyä¸»é¡¹ç›®çš„å…¨æ–‡æœç´¢æ ¼å¼
                search_query = "æµ‹è¯•æ–‡æ¡£"
                
                # å°è¯•ä½¿ç”¨MATCH_ALLå‡½æ•°
                try:
                    search_sql = f"""
                    SELECT id, page_content, metadata
                    FROM {self.test_schema}.{self.test_collection}
                    WHERE MATCH_ALL(page_content, '{search_query}')
                    LIMIT 3
                    """
                    
                    start_time = time.time()
                    cursor.execute(search_sql)
                    results = cursor.fetchall()
                    search_time = time.time() - start_time
                    
                    print(f"âœ… å…¨æ–‡æœç´¢æˆåŠŸ: è¿”å› {len(results)} ä¸ªç»“æœï¼Œè€—æ—¶ {search_time*1000:.0f}ms")
                    
                except Exception as e:
                    # é™çº§åˆ°LIKEæœç´¢
                    print(f"â„¹ï¸ MATCH_ALLæœç´¢å¤±è´¥ï¼Œé™çº§åˆ°LIKEæœç´¢: {e}")
                    
                    search_sql = f"""
                    SELECT id, page_content, metadata
                    FROM {self.test_schema}.{self.test_collection}
                    WHERE page_content LIKE '%{search_query}%'
                    LIMIT 3
                    """
                    
                    start_time = time.time()
                    cursor.execute(search_sql)
                    results = cursor.fetchall()
                    search_time = time.time() - start_time
                    
                    print(f"âœ… LIKEæœç´¢æˆåŠŸ: è¿”å› {len(results)} ä¸ªç»“æœï¼Œè€—æ—¶ {search_time*1000:.0f}ms")
                
                # éªŒè¯æœç´¢ç»“æœ
                for i, row in enumerate(results):
                    doc_id, content, metadata_json = row
                    print(f"   ç»“æœ {i+1}: æ–‡æ¡£ID={doc_id}, å†…å®¹é•¿åº¦={len(content)}")
                
                print("âœ… å…¨æ–‡æœç´¢éªŒè¯é€šè¿‡")
                
                self.test_results['full_text_search'] = True
                return True
                
        except Exception as e:
            print(f"âŒ å…¨æ–‡æœç´¢æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['full_text_search'] = False
            return False
    
    def test_cross_project_compatibility(self) -> bool:
        """æµ‹è¯•è·¨é¡¹ç›®å…¼å®¹æ€§"""
        print("\nğŸ§ª æµ‹è¯•è·¨é¡¹ç›®å…¼å®¹æ€§...")
        
        try:
            with self.connection.cursor() as cursor:
                # éªŒè¯è¡¨ç»“æ„æ˜¯å¦å…¼å®¹difyä¸»é¡¹ç›®
                cursor.execute(f"DESC {self.test_schema}.{self.test_collection}")
                columns = cursor.fetchall()
                
                # æ£€æŸ¥å­—æ®µç±»å‹
                field_types = {col[0]: col[1] for col in columns}
                
                expected_types = {
                    "id": "STRING",
                    "page_content": "STRING", 
                    "metadata": "JSON",
                    "vector": "VECTOR"
                }
                
                compatibility_issues = []
                for field, expected_type in expected_types.items():
                    if field not in field_types:
                        compatibility_issues.append(f"ç¼ºå°‘å­—æ®µ: {field}")
                    elif expected_type not in field_types[field]:
                        compatibility_issues.append(f"å­—æ®µç±»å‹ä¸åŒ¹é…: {field} (æœŸæœ›: {expected_type}, å®é™…: {field_types[field]})")
                
                if compatibility_issues:
                    print(f"âŒ å…¼å®¹æ€§é—®é¢˜: {', '.join(compatibility_issues)}")
                    return False
                
                print("âœ… è·¨é¡¹ç›®å…¼å®¹æ€§éªŒè¯é€šè¿‡")
                print(f"   è¡¨ç»“æ„å®Œå…¨å…¼å®¹difyä¸»é¡¹ç›®")
                
                self.test_results['cross_project_compatibility'] = True
                return True
                
        except Exception as e:
            print(f"âŒ è·¨é¡¹ç›®å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['cross_project_compatibility'] = False
            return False
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        try:
            if self.connection:
                with self.connection.cursor() as cursor:
                    cursor.execute(f"DROP TABLE IF EXISTS {self.test_schema}.{self.test_collection}")
                print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†è­¦å‘Š: {e}")
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š Clickzettaä¸€è‡´æ€§æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        passed_tests = sum(1 for result in self.test_results.values() if result)
        total_tests = len(self.test_results)
        
        print(f"æµ‹è¯•é›†åˆ: {self.test_collection}")
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
        print(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
        print(f"é€šè¿‡ç‡: {(passed_tests/total_tests)*100:.1f}%")
        
        print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for test_name, result in self.test_results.items():
            status_icon = "âœ…" if result else "âŒ"
            print(f"  {status_icon} {test_name}: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        
        # æ€»ç»“
        if passed_tests == total_tests:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¸¤ä¸ªé¡¹ç›®çš„Clickzettaå®ç°å®Œå…¨ä¸€è‡´ã€‚")
        else:
            print(f"\nâš ï¸ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ã€‚")
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': (passed_tests/total_tests)*100,
            'results': self.test_results
        }
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹Clickzettaä¸€è‡´æ€§æµ‹è¯•")
        print("="*60)
        
        # å»ºç«‹è¿æ¥
        if not self.setup_connection():
            print("âŒ è¿æ¥å¤±è´¥ï¼Œæµ‹è¯•ä¸­æ­¢")
            return None
        
        try:
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            tests = [
                self.test_table_structure_consistency,
                self.test_index_creation_consistency,
                self.test_data_insertion_consistency,
                self.test_vector_search_consistency,
                self.test_full_text_search_consistency,
                self.test_cross_project_compatibility
            ]
            
            for test in tests:
                test()
            
            # ç”ŸæˆæŠ¥å‘Š
            return self.generate_report()
            
        finally:
            self.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    try:
        test = ClickzettaConsistencyTest()
        report = test.run_all_tests()
        
        if report and report['success_rate'] >= 80:
            print(f"\nğŸ¯ æµ‹è¯•å®Œæˆï¼æˆåŠŸç‡: {report['success_rate']:.1f}%")
            print("âœ… ä¸¤ä¸ªé¡¹ç›®çš„Clickzettaå®ç°åŸºæœ¬ä¸€è‡´")
        else:
            print(f"\nğŸ¯ æµ‹è¯•å®Œæˆï¼æˆåŠŸç‡: {report['success_rate']:.1f}%")
            print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ä»¥æé«˜ä¸€è‡´æ€§")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    main()